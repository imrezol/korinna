## word vectors (word embedding)
* vector
* length of vector
* GloVe vectors
* word2vec
* similar words
* analogy : king - man + woman = queen
* New York -> New_York
* https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf
* One of the main limitations of word embeddings (word vector space models in general) is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space).
    ```
    “The club I tried yesterday was great!”
    ```
    * It is not clear if the term club is related to the word sense of a club sandwich, baseball club, clubhouse, golf club, or any other sense that club might have.
    
    
* word sensors
* word sense
* word couting vs capturing meaning
* word counting
    * unigram couns
    
context
one word in a matrix is a row
nokia close to samsung but close to finland
a word can be close a lot of other words in different direction
qoed2vec one word -> 2 vectors (center word + outside words)
